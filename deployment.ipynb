{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd736dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required dependencies\n",
    "import re\n",
    "import socket\n",
    "import requests\n",
    "from urllib.parse import urlparse, parse_qs\n",
    "from bs4 import BeautifulSoup\n",
    "import joblib\n",
    "import warnings\n",
    "import pandas as pd\n",
    "from flask import Flask, request, jsonify"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef92c9a5",
   "metadata": {},
   "source": [
    "In a real world scenario, it is desired to be able to just query an URL and have the model do the prediction. Let's start by making a function capable of extrating the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f0df6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_url_features(url):\n",
    "    features = {}\n",
    "\n",
    "    # Passive features extraction\n",
    "    parsed = urlparse(url)\n",
    "    hostname = parsed.hostname or \"\"\n",
    "    path = parsed.path or \"\"\n",
    "    query = parsed.query or \"\"\n",
    "\n",
    "    features['NumDots'] = url.count('.')\n",
    "    features['SubdomainLevel'] = hostname.count('.') if hostname else 0\n",
    "    features['PathLevel'] = path.count('/') if path else 0\n",
    "    features['UrlLength'] = len(url)\n",
    "    features['NumDash'] = url.count('-')\n",
    "    features['NumDashInHostname'] = hostname.count('-') if hostname else 0\n",
    "    features['NumQueryComponents'] = len(parse_qs(query))\n",
    "    features['NumDigits'] = sum(c.isdigit() for c in url)\n",
    "    features['NoHttps'] = 1 if parsed.scheme == 'https' else 0\n",
    "    try: \n",
    "        socket.inet_aton(hostname)\n",
    "        features['IpAddress'] = 1\n",
    "    except:\n",
    "        features['IpAddress'] = 0\n",
    "\n",
    "    features['TildeSymbol'] = 1 if '~' in url else 0\n",
    "    features['NumUnderscore'] = url.count('_')\n",
    "    features['NumAmpersand'] = url.count('&')\n",
    "    features['RandomString'] = 1 if re.search(r'[a-zA-Z0-9]{10,}', url) else 0\n",
    "    parts = hostname.split('.') if hostname else []\n",
    "    features['DomainInPaths'] = 1 if hostname and parts[-1] in path else 0\n",
    "    features['HostnameLength'] = len(hostname)\n",
    "    features['PathLength'] = len(path)\n",
    "    features['QueryLength'] = len(query)\n",
    "    sensitive_words = ['login', 'secure', 'account', 'update', 'bank', 'verify', 'signin', 'password', 'confirm', 'ebayisapi', 'webscr', 'paypal']\n",
    "    features['NumSensitiveWords'] = sum(word in url.lower() for word in sensitive_words)\n",
    " \n",
    "\n",
    "    # Active features extraction\n",
    "    try:\n",
    "        head = requests.head(url, timeout=5, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "        size = int(head.headers.get('Content-Length', 0))\n",
    "        if size and size < 100_000_000:\n",
    "            resp = requests.get(url, timeout=5, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "            if resp.status_code == 200:\n",
    "                soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "\n",
    "                links = soup.find_all('a', href=True)\n",
    "                if links:\n",
    "                    ext_links = [link for link in links if hostname not in urlparse(str(link.get('href') or '')).netloc]\n",
    "                    features['PctExtHyperlinks'] = len(ext_links) / len(links)\n",
    "                else:\n",
    "                    features['PctExtHyperlinks'] = 0\n",
    "                \n",
    "                resources = soup.find_all(['img', 'script', 'link'], src=True)\n",
    "                if resources:\n",
    "                    ext_resources = [res for res in resources if hostname not in urlparse(str(res.get('src') or res.get('href') or '')).netloc]\n",
    "                    features['PctExtResourceUrls'] = len(ext_resources) / len(resources)\n",
    "                else:\n",
    "                    features['PctExtResourceUrls'] = 0\n",
    "                \n",
    "                null_links = [link for link in links if link['href'] in ('', '#', '/', url, 'javascript::void(0)', 'JavaScript:: void(0)')]\n",
    "                features['PctNullSelfRedirectHyperlinks'] = len(null_links) / len(links) if links else 0\n",
    "\n",
    "                features['FrequentDomainNameMismatch'] = 1 if features['PctExtHyperlinks'] > 0.5 else 0\n",
    "\n",
    "                forms = soup.find_all('form')\n",
    "                features['SubmitInfoToEmail'] = 1 if any('mailto:' in str(form) for form in forms) else 0\n",
    "\n",
    "                metas = soup.find_all(['meta', 'script', 'link'])\n",
    "                if metas:\n",
    "                    ext_metas = [meta for meta in metas if hostname not in urlparse(str(meta.get('src') or meta.get('href') or '')).netloc]\n",
    "                    ratio = len(ext_metas) / len(metas)\n",
    "                    if ratio > 0.66:\n",
    "                        features['ExtMetaScriptLinkRT'] = 1\n",
    "                    elif ratio > 0.33:\n",
    "                        features['ExtMetaScriptLinkRT'] = 0\n",
    "                    else:\n",
    "                        features['ExtMetaScriptLinkRT'] = -1\n",
    "                else:\n",
    "                    features['ExtMetaScriptLinkRT'] = None\n",
    "\n",
    "                features['InsecureForms'] = 1 if any(form.get('action', '').startswith('http://') for form in forms) else 0\n",
    "                features['IframeOrFrame'] = 1 if soup.find_all(['iframe', 'frame']) else 0\n",
    "\n",
    "                if links:\n",
    "                    suspicious = 0\n",
    "                    for link in links:\n",
    "                        href = str(link.get('href') or '').lower()\n",
    "                        netloc = urlparse(href).netloc.lower()\n",
    "                        if netloc and hostname.lower() not in netloc:\n",
    "                            suspicious += 1\n",
    "                        elif href.startswith('#'):\n",
    "                            suspicious += 1\n",
    "                        elif 'javascript::void(0)' in href:\n",
    "                            suspicious += 1\n",
    "                    pcd = suspicious / len(links)\n",
    "                    if pcd > 0.66:\n",
    "                        features['PctExtNullSelfRedirectHyperlinksRT'] = 1\n",
    "                    elif pcd > 0.33:\n",
    "                        features['PctExtNullSelfRedirectHyperlinksRT'] = 0\n",
    "                    else:\n",
    "                        features['PctExtNullSelfRedirectHyperlinksRT'] = -1            \n",
    "            else:\n",
    "                features['PctExtHyperlinks'] = None\n",
    "                features['PctExtResourceUrls'] = None\n",
    "                features['PctNullSelfRedirectHyperlinks'] = None\n",
    "                features['FrequentDomainNameMismatch'] = None\n",
    "                features['SubmitInfoToEmail'] = None\n",
    "                features['ExtMetaScriptLinkRT'] = None\n",
    "                features['InsecureForms'] = None\n",
    "                features['IframeOrFrame'] = None\n",
    "                features['PctExtNullSelfRedirectHyperlinksRT'] = None\n",
    "        else:\n",
    "                features['PctExtHyperlinks'] = None\n",
    "                features['PctExtResourceUrls'] = None\n",
    "                features['PctNullSelfRedirectHyperlinks'] = None\n",
    "                features['FrequentDomainNameMismatch'] = None\n",
    "                features['SubmitInfoToEmail'] = None\n",
    "                features['ExtMetaScriptLinkRT'] = None\n",
    "                features['InsecureForms'] = None\n",
    "                features['IframeOrFrame'] = None\n",
    "                features['PctExtNullSelfRedirectHyperlinksRT'] = None\n",
    "    except:\n",
    "        features['PctExtHyperlinks'] = None\n",
    "        features['PctExtResourceUrls'] = None\n",
    "        features['PctNullSelfRedirectHyperlinks'] = None\n",
    "        features['FrequentDomainNameMismatch'] = None\n",
    "        features['SubmitInfoToEmail'] = None\n",
    "        features['ExtMetaScriptLinkRT'] = None\n",
    "        features['InsecureForms'] = None\n",
    "        features['IframeOrFrame'] = None\n",
    "        features['PctExtNullSelfRedirectHyperlinksRT'] = None\n",
    "\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae34f536",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = joblib.load('./models/lgbm.pkl')\n",
    "\n",
    "def classify_url(url, model=model):\n",
    "\n",
    "    try:\n",
    "        resp = requests.head(url, allow_redirects=True, timeout=5)\n",
    "        if resp.status_code == 200:\n",
    "            status = 'OK'\n",
    "        else:\n",
    "            status = 'UNREACHABLE'\n",
    "    except:\n",
    "        status = 'UNREACHABLE'\n",
    "\n",
    "    features = extract_url_features(url)\n",
    "\n",
    "    expected_features = model.feature_names_in_\n",
    "    X_input = pd.DataFrame([[features.get(f, None) for f in expected_features]], columns=expected_features)\n",
    "    \n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter('ignore', category=UserWarning)\n",
    "        prediction = model.predict(X_input)[0]\n",
    "        proba = model.predict_proba(X_input)[0][prediction]\n",
    "\n",
    "    return {'status': status, 'label': 'PHISHING' if prediction == 1 else 'LEGIT', 'probability': float(round(proba, 3))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8cfdca78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'OK', 'label': 'LEGIT', 'probability': 0.973}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify_url('https://facebook.com/login/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e643763",
   "metadata": {},
   "source": [
    "Some common legitimate URLs were tested and the results were good. To test with real malicious URL's, a sample from the phishtank dataset was used (available at: https://www.phishtank.com/developer_info.php)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "57c7c642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positives: 112 (74.67%) (54 unreachable)\n",
      "Negatives: 38 (25.33%) (18 unreachable)\n"
     ]
    }
   ],
   "source": [
    "file_path = './files/verified_online.csv'\n",
    "df = pd.read_csv(file_path, usecols=['url'])\n",
    "\n",
    "sampled_df = df.head(10000).sample(n=150, random_state=42)\n",
    "\n",
    "positive = 0\n",
    "positive_unreachable = 0\n",
    "negative = 0\n",
    "negative_unreachable = 0\n",
    "\n",
    "for url in sampled_df['url']:\n",
    "    result = classify_url(url)  # returns dict\n",
    "    label = result['label']\n",
    "    status = result['status']\n",
    "    if label == 'LEGIT':\n",
    "        positive += 1\n",
    "        if status == 'UNREACHABLE':\n",
    "            positive_unreachable += 1 \n",
    "    elif label == 'PHISHING':\n",
    "        negative += 1\n",
    "        if status == 'UNREACHABLE':\n",
    "            negative_unreachable += 1 \n",
    "\n",
    "total = positive + negative\n",
    "if total > 0:\n",
    "    pct_positive = (positive / total) * 100\n",
    "    pct_negative = (negative / total) * 100\n",
    "\n",
    "    print(f'Positives: {positive} ({pct_positive:.2f}%) ({positive_unreachable} unreachable)')\n",
    "    print(f'Negatives: {negative} ({pct_negative:.2f}%) ({negative_unreachable} unreachable)')\n",
    "else:\n",
    "    print('No valid URLs to process.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1053c7",
   "metadata": {},
   "source": [
    "For day-to-day usage, a more practical way to use the model would be nice. Just as a proof of concept of how the model could be applied, let's make a very basic web app that will take the URL as query and provide the classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "337ee3c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
      " * Running on http://127.0.0.1:5001\n",
      "\u001b[33mPress CTRL+C to quit\u001b[0m\n",
      "127.0.0.1 - - [18/Dec/2025 23:15:38] \"GET /classify?url=https://www.facebook.com/login HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "app = Flask(__name__)\n",
    "@app.route('/classify', methods=['GET'])\n",
    "def classify():\n",
    "    url = request.args.get('url')\n",
    "    if not url:\n",
    "        return jsonify({\"error\": \"URL parameter is missing\"}), 400\n",
    "    \n",
    "    result = classify_url(url)\n",
    "    return jsonify({\"url\": url, \"classification\": result})\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(debug=True, port=5001, use_reloader=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.14.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
